<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <style>
    div.padded {
      padding-top: 30px;
      padding-right: 100px;
      padding-bottom: 0.25in;
      padding-left: 100px;
      margin: 60px;
      background-color: white;
    }

    body {
      font-family: "Nunito", sans-serif;
      font-size: 1.1rem;
      background: linear-gradient(to top left, #33ccff 0%, #ff99cc 100%);
      line-height: 170%;
    }

    /* h2 {
      text-align: center;
    } */

    td {
      padding: 25px;
    }

    .variable {
      font-family: monospace;
    }

    .left-img {
      float: left;
      margin-right: 30px;
      margin-bottom: 10px;
    }

    .right-txt {
      clear: right;
    }


    td {
      padding: 5px;
    }

    .equation {
      margin: 0 auto 0 auto;
    }

    .c {
      border-collapse: collapse;
    }

    img {
      width: 350px;
      height: 350px;
    }
    h1 {
        text-align: center;
    }
    table {
      align-content: center;
    }
  </style>
  <title>CS 184 Final Project: Light Field Camera</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <link rel="stylesheet" type="text/css" href="style.css" media="screen" />
  <link href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital@0;1&family=Nunito:wght@400;600&display=swap" rel="stylesheet">
</head>

<body>
    <h1 align="middle">CS184 Final Project</h1>
    <h1 align="middle">Weiyuan Chen, Shengxian Chen, Yiming wang, ChingYi Leung
    <h1>Repo: https://github.com/joewychen/suibian-final-pj</h1>

    <div class="padded">
        <h1>Light Field Camera Support</h1>
        <h2>Abstract</h2>
        <p>In our project, we implement light field camera support such that we can focus on 
          differet position of the image without rendering the image again. This project requires
          light field data (a set of images), so we also use the pathtracer renderer to collect
          our light field data. We also speed up our computation via GPU. Finally, We visualize our
          results with a GUI which can refocus the image based on user's input of the alpha value
          and the radius of the aperture.
        </p>
        We spilt our project into three parts.
        <ol>
          <li>Collecting light field data</li>
          <li>Image refocusing with different aperatures</li>
          <li>GUI support</li>
        </ol>

        <h2>Technical approach</h2>
        <h3>How do we collect our light field data?</h3>
        <p>We use our course project, Proj3-1, as the renderer of our data generator.
           We used a python script in which generate different images with different camera positons.
          The python script import a camerapos file and write different camera position, so that the image generated from this file
            will have a position goes from left to right, from top to the bottom. Also, it's important
          to label the position in the image file name because our application requires that information
        in order to produce correct refocusing. We also download some realistic light field data from
      <a href="http://lightfield.stanford.edu/lfs.html">stanford archive</a>.</p>
      <table>
        <tr>
          <td>
            <img src="./images/data_collect.jpg" style="width:450px;height:400px;">
            <p>From pathtracer renderer</p>
          </td>
          <td>
            <img src="./images/stanfordar.png" style="width:450px;height:400px;">
            <p>From stanford archive</p>
          </td>
        </tr>
      </table>
              
        <h3>How does refocusing work?</h3>
        <div style="text-align: center;">
          <img src="./images/refous.jpg" style="width:550px;height:300px;"> 
          <p>Fig.1</p>
        </div>
        <p>Imagine we have a grid of small lens, where each len contains a small image of the scene with different angles and position. 
          Each len can be seen as some rotation of the middle len, we can think of them as two view points:

          <br>
          Suppose there is a far object & a close object in the scene, (Fig.1)
          if we shift our camera position by an angle alpha, D(obj moves in camera) = tan(alpha) * (FOV - d), 
          where d is the object distance to the main len, and alpha is the angle between original camera vertical line as well as the shifted camera vertical line.
          I.e. if d is large, meaning far object, (FOV - d) will be small, so D will also be small.
          If d is large, meaning close object, (FOV - d) will be large, so D will also be large.
          <br>
          Therefore, averaging all images from different viewpoints can be visualize as the many overlapping of far object, 
          because far object won't provide large shifting. While the far objects keeps overlapping, the close objects will be blurry, 
          this is because close objects are highly affected by the shifting, therefore, we could imagine a scene that far object are clear, and
          close objects are blurry, which is a refocusing effect particular on far obj. As shown in the following image:
          </p>

          <div align="center">
            <img src="./images/chess_0.png" style="width:550px;height:350px;">
          <p> alpha = 0, focus at natural offset</p>
          </div>

          <p>Given all these images with different shifting, we can consider provide different shifting on it so that it can achieve
            what refocusing onto different objects within different distances. In this case, we created a 17 by 17 grid, which stores
            image in diffenent camera positions and angles. Based on different images, we choose to shift them towards the central image, 
            which is the (8, 8) image. We use np.roll to do the shifting against each image based on the position of the image in the grid, 
            i.e. say a is right at (0, 0), we will then shift the image into (8, 8). This will make every image getting close to the center.
            If we think about the result of this, this will simply be changing the different overlapping area as opposed to merely averaging 
            each image. As a result, when alpha = 1, we can see that we have a different refocusing effect.
          </p>
          <div align="center">
            <img src="./images/chess_1.png" style="width:550px;height:350px;">
            <p> alpha = 1, focus at more front of the scene</p>
          </div>


          <p>As opposed to these two images above, the image with alpha = -1 will be shown as below, which focus on the further object of the image. </p>

          <div align="center">
            <img src="./images/chess_-1.png" style="width:550px;height:350px;">
            <p> alpha = -1, focus at more back of the scene</p>
          </div>
          <h3>How does aperture adjustment work?</h3>
          <p> Larger aperture means more lights get into the camera, and the blurry effects are stronger.
            In order to simulate this effects, we include more light field data for the refocusing. We first
            pick the (8,8) grid as the center of our 17 by 17 grids. Then, we expand the size from just one
            light field data at (8, 8) to 289 size light field data based on the radius. Including more light
            field images and averaging them after shifting can get more blurry results because more rgb values
            are added up to produce more difference in rgb value.
          </p>

          <table>
            <tr>
              <td>
                <img src="./images/drag0.png" alt="" >
                <p>radius=0, is just act like a pinhole camera</p>
              </td>
              <td>
                <img src="./images/drag4.png" alt="" >
                <p>radius=4, we see the blurry effect</p>
              </td>
              <td>
                <img src="./images/drag8.png" alt="" >
                <p>radius=8, the most blurry effect in our case</p>
              </td>
            </tr>
          </table>


          <h3> Acceleration we used on the project. </h3>
          <p> Since refocusing requires lots of matrix operations, we run our matrix operation on GPU with CUPU.
             It took 10s to 12s on my Macbook pro without speedup, and now only take 1s to 2s on the GPU computer.
          </p>
          <p> We wrote our immediate result from the np.array that was produced by different image set into a file (permanent caching), this reduces
            the lading images process time from 20s to 1s each. 
          </p>

          <h2> GUI support </h2>
          <p>GUI can let user to choose different scene, alpha value, and radius of the aperture.
            The GUI did not precalculate any image, we calculate our refocusing at real time and show
            the image on the GUI. Therefore, speed of the calculation is very important in our project.
          </p>
          <div align="center"> <img src="./images/gui.png" style="width:650px;height:400px;"></div>

        <h2>Results</h2>
        <table align="center">
          <tr>
            <td>
              <img src="./images/blucy.gif" alt="" >
              <p>changing alpha from -2 to 2</p>
            </td>
            <td>
              <img src="./images/dragon_ref.gif" alt="" >
              <p>changing alpha from -2 to 2</p>
            </td>
          </tr>
          <tr>
            <td>
              <img src="./images/dragon_ape.gif" alt="" >
              <p>chaning radius from 0 to 8</p>
            </td>
            <td>
              <img src="./images/knight.gif" alt="" >
              <p>More from stanford light field data</p>
            </td>
          </tr>
          <tr>
            <td>
              <img src="./images/chess.gif" alt="" >
            </td>
            <td>
              <img src="./images/ball.gif" alt="" >
            </td>
          </tr>
            <tr>
              <td>
                <img src="./images/bean.gif" alt="" >
              </td>
              <td>
                <img src="./images/bracelet.gif" alt="" >
              </td>
            </tr>
        </table>

        <h2>References</h2>
        <ul>
          <li>
            <a href="https://inst.eecs.berkeley.edu/~cs194-26/fa18/upload/files/proj5/cs194-26-abr/">CS194 Writeup</a>
          </li>
          <li>
            <a href="https://graphics.stanford.edu/papers/lfcamera/lfcamera-150dpi.pdf">Light Field Photography with a Hand-held Plenoptic Camera</a>
          </li>
          <li>
            <a href="http://lightfield.stanford.edu/">Stanford light field image archive</a>
          </li>
          <li>
            <a href="https://inst.eecs.berkeley.edu/~cs194-26/fa18/hw/proj5/index.html"> Reference spec </a> 
          </li>

        </ul>
        <h2>Contributions from each team member</h2>
        <p> Most part of the project are working in group, and Weiyuan Chen provides visual studio code live sharing between groupmates</p>
        <p> Weiyuan Chen</p> 
        <p> I implemented the general structure of the refocusing code, but it still needs some improvement. Also, I implemented
          the whole aperture processing. I also implemented the basic structure of the GUI.
        </p> 
        <p> Shengxian Chen</p> 
        <p>
          I finished up the rest of the refocusing part, which supports different image layouts and camera position handling. Gave idea about the
          acceleration of the preprocesssing part (reading image into permanent cache). 
        </p>
        <p> Yiming wang</p>
        <p>I implemented the data collection and the GUI support.</p>
        <p> ChingYi Leung</p>
        <p>I did the refocusing, videos, and writeup.</p>

    </div>

</body>

</html>
